# Scripts

Scripts d'orchestration pour ALFRED experiments.

## ğŸ“ Structure

```
scripts/
â”œâ”€â”€ training/          # EntraÃ®nement de modÃ¨les
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ train.sh
â”œâ”€â”€ evaluation/        # Ã‰valuation de modÃ¨les
â”‚   â”œâ”€â”€ eval_best.sh
â”‚   â””â”€â”€ eval_with_env.sh
â”œâ”€â”€ analysis/          # Analyse de rÃ©sultats
â”‚   â”œâ”€â”€ compare.sh
â”‚   â””â”€â”€ analyze.sh
â”œâ”€â”€ maintenance/       # Maintenance du projet
â”‚   â”œâ”€â”€ patches/       # Patches pour ALFRED
â”‚   â””â”€â”€ cleanup/       # Nettoyage
â””â”€â”€ utils/             # Utilitaires
    â”œâ”€â”€ list_experiments.sh
    â””â”€â”€ check_status.sh
```

## ğŸš€ Workflows Typiques

### 1. EntraÃ®ner un ModÃ¨le

```bash
# Avec Python (recommandÃ©)
python training/run_experiment.py --config ../configs/react/react_light_v1.yaml

# Avec bash wrapper
./training/train.sh
```

### 2. Ã‰valuer un ModÃ¨le

```bash
# Ã‰valuer le meilleur checkpoint
./evaluation/eval_best.sh react_light_v1_20241209_150000

# Ã‰valuer avec environnement complet
./evaluation/eval_with_env.sh \
    ../experiments/react/*/checkpoints/best_seen.pth \
    valid_seen
```

### 3. Analyser les RÃ©sultats

```bash
# Comparer deux expÃ©riences
./analysis/compare.sh \
    ../experiments/cot/test_quick_gpu_* \
    ../experiments/react/react_light_v1_*

# Analyse dÃ©taillÃ©e
./analysis/analyze.sh react_light_v1_20241209_150000
```

### 4. Maintenance

```bash
# Appliquer patches ALFRED
./maintenance/patches/patch_baseline_gpu.sh

# Nettoyer expÃ©riences Ã©chouÃ©es
./maintenance/cleanup/clean_failed.sh

# VÃ©rifier status
./utils/check_status.sh
```

## ğŸ”— Relation avec tools/

Les scripts dans `scripts/` orchestrent des workflows complets.
Ils utilisent les outils dans `tools/` comme composants.

**Exemple:**
```bash
# scripts/analysis/compare.sh appelle:
python ../tools/analysis/compare_cot_react.py
```

## ğŸ“š Documentation

Chaque sous-dossier contient un README.md dÃ©taillÃ©.
