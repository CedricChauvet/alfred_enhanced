# Chain of Thoughts (CoT) pour ALFRED

**Auteur** : Cedrix  
**Date** : 2025  
**Base** : ALFRED Benchmark

---

## üéØ Objectif

Am√©liorer les performances du mod√®le baseline ALFRED en introduisant une g√©n√©ration explicite de **subgoals** (sous-objectifs) pour am√©liorer le planning et la d√©composition des t√¢ches.

---

## üìä Exp√©riences

### CoT v1
- **Configuration** : `configs/cot_v1.yaml`
- **Mod√®le** : `alfred_experiments.models.seq2seq_cot`
- **Ajouts** : G√©n√©ration explicite de subgoals
- **Objectif** : Am√©liorer planning et d√©composition des t√¢ches

### CoT v2
- **Configuration** : `configs/cot_v2_exploration.yaml`
- **Variations** : Plus de subgoals, loss weight plus √©lev√©
- **Objectif** : Explorer les hyperparam√®tres CoT

----------------------------------------

## üöÄ Training :


Voici le Yaml pour un entrainement du CoT: 
[Configuration CoT v1](https://github.com/CedricChauvet/alfred_enhanced/blob/main/configs/cot_v1.yaml)



```bash

# Pour un train:
cd $ALFRED_ROOT
./scripts/train.sh ./config/cot_v1.yaml
```
---------------------------------------
## üìä Monitoring avec TensorBoard

### Qu'est-ce que TensorBoard ?

TensorBoard est l'outil de visualisation de TensorFlow/PyTorch qui permet de suivre en temps r√©el l'entra√Ænement de vos mod√®les. Il affiche :

- **Courbes de loss** : √âvolution des pertes d'entra√Ænement et de validation
- **M√©triques** : Accuracy, Success Rate, Goal Condition, etc.
- **Graphes** : Architecture du r√©seau de neurones
- **Histogrammes** : Distribution des poids et gradients
- **Images** : Visualisation des pr√©dictions (optionnel)

### Lancement de TensorBoard
```bash
# Depuis n'importe quel terminal
tensorboard --logdir /chemin/vers/experiments/nom_experience/tensorboard
```

### Acc√®s √† l'interface

Une fois lanc√©, TensorBoard affiche :
```
TensorBoard 2.x.x at http://localhost:6006/ (Press CTRL+C to quit)
```
--------------------------------------

## üß™ √âvaluation

### √âvaluation sur validation seen
```bash
python models/eval/eval_seq2seq.py 
--model_path experiments/cot_v1/best_seen.pth 
--eval_split valid_seen 
--data data/json_feat_2.1.0 
--model alfred_experiments.models.seq2seq_cot 
--gpu 
--num_threads 2
```

Devrait lancer Thor


----------------------------------------
## üèóÔ∏è Architecture

----------------------------------------

### Vue d'ensemble CoT v1
Le mod√®le CoT v1 ajoute **5 nouveaux layers** (~252K param√®tres) au mod√®le baseline (~10M param√®tres) :

1. **project_cont** - Projection du contexte linguistique
2. **project_subgoals** - Projection des subgoals
3. **subgoal_decoder** - LSTM pour g√©n√©ration auto-r√©gressive
4. **subgoal_classifier** - Classification des subgoals
5. **emb_subgoal** - Embedding des indices de subgoals

En r√©sum√©, l'ajout par rapport a la baseline et de cr√©er la CoT qui est une liste d'actions de ce type:

Exemple 1 : "Put a heated apple in the fridge"
Subgoals (high-level actions) :

GotoLocation (Counter/Table)
PickupObject (Apple)
GotoLocation (Microwave)
PutObject (Apple in Microwave)
ToggleObject (Microwave ON)
ToggleObject (Microwave OFF)
PickupObject (Apple from Microwave)
GotoLocation (Fridge)
OpenObject (Fridge)
PutObject (Apple in Fridge)


Cette liste est concat√©n√©e avec la sortie de l'encoder

----------------------------------------



### Vue d'ensemble CoT_ProgressMonitor with Attention

Le modele CoT_pm_attention est un peu plus √©labor√©:

Il utilise l'apprentissage du progress monitor qui indique l'avanc√©e des subgaols en pourcentage.

Par exemple 0% aucun subgoals atteints 50% la moiti√© de la tache est remplie.

coupl√© avec le CoT, ce modele est capable de predire quelle tache actuelle l'IA doit resoudre.


----------------------------------------


### Differences entre v1 et pm_attention
Tout est  dans la taille de l'encodage, v1 concat√®ne un vecteur de taille max_subgoals=12

pm_attention concatene lui aussi a la sortie de l'encodeur mais seulement un √©l√©ment (par exemple go to location, ou pickup)

En r√©sume le modele sait ce qu'il doit faire a chaque instant.



----------------------------------------