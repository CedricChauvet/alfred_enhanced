import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from models.model.seq2seq_im_mask import Module as BaseModule
from models.nn import vnn
import numpy as np


class Module(BaseModule):
    """
    Mod√®le avec pr√©diction Chain-of-Thought des subgoals
    H√©rite du mod√®le baseline seq2seq_im_mask
    """
    
    def __init__(self, args, vocab):
        """
        Initialisation du mod√®le avec d√©codeur de subgoals
        """
        super().__init__(args, vocab)
        
        # Vocabulaire des subgoals (actions high-level)
        self.vocab_subgoal = vocab['action_high']
        
        # Dimension des embeddings de subgoals
        self.demb_subgoal = getattr(args, 'demb_subgoal', args.demb)
        
        # Embeddings des subgoals
        self.emb_subgoal = nn.Embedding(len(self.vocab_subgoal), self.demb_subgoal)
        
        # D√©codeur LSTM pour les subgoals
        # Prend en entr√©e: embedding du subgoal pr√©c√©dent + contexte linguistique
        self.subgoal_decoder = nn.LSTM(
            input_size=self.demb_subgoal + 2*args.dhid,  # embedding + contexte bidirectionnel
            hidden_size=args.dhid,
            num_layers=1,
            batch_first=True
        )
        
        # Couche de projection pour pr√©dire le subgoal suivant
        self.subgoal_proj = nn.Linear(args.dhid, len(self.vocab_subgoal))
        
        # Embedding GO pour d√©marrer la g√©n√©ration (comme dans le d√©codeur d'actions)
        self.subgoal_go = nn.Parameter(torch.Tensor(self.demb_subgoal))
        nn.init.normal_(self.subgoal_go)
        
        # Utiliser les tokens existants du vocabulaire parent
        # self.pad et self.stop_token sont d√©j√† d√©finis dans la classe Base
        
        # Dropout pour le d√©codeur de subgoals
        self.subgoal_dropout = nn.Dropout(getattr(args, 'subgoal_dropout', 0.1))
        
        # Attention pour combiner contexte linguistique et subgoals
        self.subgoal_attention = vnn.SelfAttn(args.dhid)
        
        # Param√®tre pour activer/d√©sactiver la g√©n√©ration de subgoals
        self.use_subgoals = getattr(args, 'use_subgoals', True)
        
        print(f"‚úÖ Subgoal decoder initialized with vocab size: {len(self.vocab_subgoal)}")
    
    
    def generate_subgoals(self, enc_lang, cont_lang, max_subgoals=10, sampling=False, temperature=1.0):
        """
        G√©n√®re une s√©quence de subgoals avec approche Chain-of-Thought
        
        Args:
            enc_lang: Encodage linguistique complet [batch, seq_len, dhid*2]
            cont_lang: Contexte linguistique (dernier √©tat cach√©) [batch, dhid*2]
            max_subgoals: Nombre maximum de subgoals √† g√©n√©rer
            sampling: Si True, √©chantillonne; sinon prend argmax
            temperature: Temp√©rature pour le sampling (plus √©lev√© = plus al√©atoire)
        
        Returns:
            subgoals: Liste de subgoals pr√©dits [batch, num_subgoals]
            subgoal_logits: Logits pour chaque subgoal [batch, num_subgoals, vocab_size]
            subgoal_embeddings: Embeddings des subgoals [batch, num_subgoals, demb_subgoal]
        """
        device = next(self.parameters()).device
        batch_size = enc_lang.size(0)
        
        # Liste pour stocker les pr√©dictions
        predicted_subgoals = []
        subgoal_logits_list = []
        subgoal_embeddings_list = []
        
        # √âtat cach√© initial du d√©codeur (d√©riv√© du contexte linguistique)
        # R√©duire la dimension de cont_lang (dhid*2) vers dhid
        h_t = cont_lang[:, :self.args.dhid].unsqueeze(0).contiguous()  # [1, batch, dhid]
        c_t = torch.zeros_like(h_t)  # √âtat de cellule initial
        
        # Embedding de d√©marrage GO pour tous les exemples du batch
        emb_t = self.subgoal_go.repeat(batch_size, 1)  # [batch, demb_subgoal]
        current_token = None  # Premi√®re it√©ration utilise GO, pas un token
        
        # G√©n√©ration autoregressif des subgoals
        for step in range(max_subgoals):
            # Embedding du token actuel (GO pour la premi√®re it√©ration, sinon embedding du token pr√©c√©dent)
            if current_token is None:
                # Premi√®re it√©ration: utiliser GO
                emb_current = emb_t
            else:
                # It√©rations suivantes: utiliser l'embedding du token pr√©dit
                emb_current = self.emb_subgoal(current_token)  # [batch, demb_subgoal]
            
            # Concat√©ner avec le contexte linguistique
            decoder_input = torch.cat([emb_current, cont_lang], dim=-1)  # [batch, demb_subgoal + dhid*2]
            decoder_input = decoder_input.unsqueeze(1)  # [batch, 1, demb_subgoal + dhid*2]
            
            # Passer dans le LSTM d√©codeur
            lstm_out, (h_t, c_t) = self.subgoal_decoder(decoder_input, (h_t, c_t))
            
            # Appliquer dropout
            lstm_out = self.subgoal_dropout(lstm_out.squeeze(1))  # [batch, dhid]
            
            # Pr√©dire le prochain subgoal
            logits = self.subgoal_proj(lstm_out)  # [batch, vocab_size]
            
            # √âchantillonnage ou argmax
            if sampling:
                # √âchantillonnage avec temp√©rature
                probs = F.softmax(logits / temperature, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1).squeeze(-1)
            else:
                # Greedy decoding
                next_token = logits.argmax(dim=-1)
            
            # Stocker les r√©sultats
            predicted_subgoals.append(next_token)
            subgoal_logits_list.append(logits)
            subgoal_embeddings_list.append(emb_current)
            
            # V√©rifier si tous les exemples du batch ont g√©n√©r√© <<stop>>
            if (next_token == self.stop_token).all():
                break
            
            # Mettre √† jour le token actuel pour le prochain step
            current_token = next_token
        
        # Stacker les r√©sultats
        subgoals = torch.stack(predicted_subgoals, dim=1)  # [batch, num_subgoals]
        subgoal_logits = torch.stack(subgoal_logits_list, dim=1)  # [batch, num_subgoals, vocab_size]
        subgoal_embeddings = torch.stack(subgoal_embeddings_list, dim=1)  # [batch, num_subgoals, demb_subgoal]
        
        return subgoals, subgoal_logits, subgoal_embeddings
    
    
    def forward(self, feat, max_decode=300):
        """
        Forward pass avec g√©n√©ration de subgoals CoT
        
        Processus:
        1. Encoder le langage (instructions + goal)
        2. G√©n√©rer les subgoals avec CoT
        3. Utiliser les subgoals pour guider la g√©n√©ration d'actions bas niveau
        """
        # Encoder le langage (appel √† la m√©thode parente)
        cont_lang, enc_lang = self.encode_lang(feat)
        
        # üéØ G√âN√âRATION DES SUBGOALS (Chain-of-Thought)
        if self.use_subgoals and not self.test_mode:
            subgoals, subgoal_logits, subgoal_embeddings = self.generate_subgoals(
                enc_lang, cont_lang, 
                max_subgoals=self.max_subgoals
            )
            
            # Stocker dans feat pour utilisation ult√©rieure et calcul de loss
            feat['predicted_subgoals'] = subgoals
            feat['subgoal_logits'] = subgoal_logits
            feat['subgoal_embeddings'] = subgoal_embeddings
        
        # Appeler le forward du mod√®le parent pour g√©n√©rer les actions bas niveau
        # (Le parent va utiliser enc_lang et cont_lang)
        out = super().forward(feat, max_decode=max_decode)
        
        # Ajouter les subgoals √† la sortie si disponibles
        if self.use_subgoals and not self.test_mode:
            out['predicted_subgoals'] = subgoals
            out['subgoal_logits'] = subgoal_logits
        
        return out
    
    
    def compute_subgoal_loss(self, predicted_logits, ground_truth_subgoals):
        """
        Calcule la loss pour la pr√©diction des subgoals
        
        Args:
            predicted_logits: Logits pr√©dits [batch, num_subgoals, vocab_size]
            ground_truth_subgoals: Subgoals ground truth [batch, num_subgoals]
        
        Returns:
            loss: Cross-entropy loss pour les subgoals
        """
        device = next(self.parameters()).device
        
        # Reshape pour le calcul de cross-entropy
        pred_flat = predicted_logits.view(-1, predicted_logits.size(-1))  # [batch*num_subgoals, vocab_size]
        gt_flat = ground_truth_subgoals.view(-1)  # [batch*num_subgoals]
        
        # Masque pour ignorer le padding
        pad_mask = (gt_flat != self.pad)
        
        # Cross-entropy loss
        loss = F.cross_entropy(pred_flat, gt_flat, reduction='none')
        loss = loss * pad_mask.float()
        loss = loss.sum() / pad_mask.sum()
        
        return loss
    
    
    def compute_loss(self, out, batch, feat):
        """
        Calcule la loss totale incluant la loss des subgoals
        """
        # Loss du mod√®le parent (actions + masques)
        losses = super().compute_loss(out, batch, feat)
        
        # Ajouter la loss des subgoals si activ√©e
        if self.use_subgoals and 'subgoal_logits' in out and 'action_high' in feat:
            subgoal_loss = self.compute_subgoal_loss(
                out['subgoal_logits'],
                feat['action_high']
            )
            
            # Poids pour la loss des subgoals
            subgoal_loss_weight = getattr(self.args, 'subgoal_loss_wt', 1.0)
            losses['subgoal'] = subgoal_loss * subgoal_loss_weight
        
        return losses


if __name__ == "__main__":
    print("‚úÖ Subgoal CoT module cr√©√© avec succ√®s!")
    print("üìù Fonctionnalit√©s:")
    print("   - generate_subgoals(): G√©n√©ration Chain-of-Thought des subgoals")
    print("   - Support du sampling et greedy decoding")
    print("   - Int√©gration avec le mod√®le baseline")