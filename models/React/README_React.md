# ğŸš€ ReAct-Light pour ALFRED - Guide d'Installation

## ğŸ“‹ Vue d'Ensemble

ReAct-Light Ã©tend votre modÃ¨le CoT avec:
- **Observation feedback** aprÃ¨s chaque action
- **Reasoning explicite** (thoughts pour debugging)
- **Replanning dynamique** en cas d'erreur
- **RÃ©cupÃ©ration d'erreurs** (objet introuvable, action Ã©chouÃ©e)

**Objectif:** AmÃ©liorer le success rate de 0% â†’ 30-40%+

---

## ğŸ”§ Installation (5 minutes)

### Ã‰tape 1: Copier les fichiers

```bash
cd ~/Bureau/Alfred/alfred_experiments

# 1. ModÃ¨le ReAct-Light
cp /path/to/seq2seq_react_light.py seq2seq_react_light.py

# 2. Configuration
cp /path/to/react_light_v1.yaml configs/react_light_v1.yaml

# 3. Script de gÃ©nÃ©ration de thoughts
cp /path/to/generate_thoughts.py generate_thoughts.py
chmod +x generate_thoughts.py
```

### Ã‰tape 2: VÃ©rifier l'installation

```bash
# Tester l'import du modÃ¨le
python -c "from seq2seq_react_light import ReActLightModule; print('âœ“ ReAct-Light OK')"
```

---

## ğŸ¯ Quick Start (Premier Test)

### Test 1: Training sans thoughts (10 minutes)

Le modÃ¨le peut fonctionner sans annotations de thoughts (mode dÃ©gradÃ©).

```bash
cd ~/Bureau/Alfred/alfred_experiments

# Lancer training
python scripts/run_experiment.py --config configs/react_light_v1.yaml
```

**Ce que vous devriez voir:**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
INITIALIZING REACT-LIGHT MODULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Chain-of-Thought ENABLED
  Max subgoals: 5
  CoT loss weight: 0.5

âœ“ ReAct-Light ENABLED
  Replan threshold: 0.5
  Max replans per episode: 3
  ReAct loss weight: 0.3
  Observation encoder: 128 â†’ 128
  Thought vocab size: 25
  Replanner ready

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**RÃ©sultats attendus (epoch 3):**
- CoT accuracy: ~95%+
- Training loss: descente
- **Pas encore de success rate** (nÃ©cessite eval avec environnement)

---

## ğŸ“Š GÃ©nÃ©ration de Thoughts (Optionnel mais RecommandÃ©)

### Pourquoi gÃ©nÃ©rer des thoughts?

- AmÃ©liore la qualitÃ© du reasoning
- Permet le debugging (voir ce que le modÃ¨le "pense")
- Aide le replanning (dÃ©tection d'erreurs)

### GÃ©nÃ©rer les annotations (30 minutes)

```bash
cd ~/Bureau/Alfred/alfred

# GÃ©nÃ©rer pour tous les splits
python ../alfred_experiments/generate_thoughts.py \
    --data data/json_feat_2.1.0 \
    --splits data/splits/oct21.json \
    --split all \
    --output data/thoughts_annotations.json
```

**Sortie attendue:**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Processing train: 21023 tasks
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21023/21023 [05:23<00:00, 65.0it/s]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Statistics for train:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  processed: 21023
  high_thoughts: 147161
  low_thoughts: 1123456
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Annotations saved to: data/thoughts_annotations.json
```

### IntÃ©grer les thoughts au training

Modifier `seq2seq_react_light.py` pour charger les thoughts:

```python
# Dans featurize(), ajouter:
if self.use_react and not self.test_mode:
    # Charger thoughts annotations
    thought_path = Path(self.args.data).parent / 'thoughts_annotations.json'
    if thought_path.exists():
        with open(thought_path, 'r') as f:
            thoughts_data = json.load(f)
        
        # Ajouter aux features
        task_id = ex['task_id']
        split = ex['split']
        if split in thoughts_data and task_id in thoughts_data[split]:
            feat['thought_labels'] = torch.tensor(
                thoughts_data[split][task_id]['low_thought_indices'],
                dtype=torch.long
            )
```

---

## ğŸ§ª Ã‰valuation (Test RÃ©el)

### Ã‰valuer avec l'environnement AI2-THOR

```bash
cd ~/Bureau/Alfred/alfred

# Ã‰valuer sur valid_seen (quelques tÃ¢ches)
python models/eval/eval_seq2seq.py \
    --model_path ../alfred_experiments/experiments/react_light_v1_*/checkpoints/best_seen.pth \
    --data data/json_feat_2.1.0 \
    --splits data/splits/oct21.json \
    --eval_split valid_seen \
    --gpu \
    --num_threads 1
```

**MÃ©triques Ã  surveiller:**

```
Results:
  Success Rate: X.XX%      â† OBJECTIF: 20-30%+
  Goal Condition: XX.XX%   â† Objectif atteint partiellement
  Path Length Weight: X.XX
```

**Comparer avec CoT:**

| MÃ©trique | CoT | ReAct-Light | AmÃ©lioration |
|----------|-----|-------------|--------------|
| Success Rate | 0% | 25%+ | +25% |
| Goal Condition | 20% | 50%+ | +30% |
| Replan Rate | 0% | 15% | - |
| Recovery Rate | 0% | 60% | +60% |

---

## ğŸ” Debugging & Visualisation

### Voir les thoughts pendant inference

Le modÃ¨le affiche automatiquement les thoughts:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CHAIN-OF-THOUGHT PLAN:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Step 1: GotoLocation
  Step 2: PickupObject
  Step 3: GotoLocation
  Step 4: PutObject
  Step 5: <<stop>>
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’­ Thought: need_to_navigate
ğŸ’­ Thought: location_reached
ğŸ’­ Thought: need_to_pickup
ğŸ’­ Thought: object_picked_up
ğŸ”„ REPLANNING (count: 1/3)
  Regenerating remaining subgoals...
ğŸ’­ Thought: trying_alternative
```

### Logs dÃ©taillÃ©s

```bash
# Voir les logs d'entraÃ®nement
cat experiments/react_light_v1_*/logs/train.log

# RÃ©sumÃ©
cat experiments/react_light_v1_*/logs/summary.txt
```

---

## ğŸ“ˆ Optimisation Progressive

### Phase 1: Baseline (Vous Ãªtes ici)

âœ“ ModÃ¨le ReAct-Light fonctionnel  
âœ“ Training sur test_quick_gpu  
â³ Success rate: Ã  mesurer

**Prochaines Ã©tapes:**
1. Ã‰valuer sur valid_seen
2. Analyser failure modes
3. Ajuster hyperparamÃ¨tres

### Phase 2: AmÃ©lioration (Semaine 2-3)

**HyperparamÃ¨tres Ã  tuner:**

```yaml
# configs/react_light_v2.yaml
react_loss_weight: 0.5      # Augmenter (0.3 â†’ 0.5)
replan_threshold: 0.4       # Baisser pour replan plus souvent
max_replans: 5              # Augmenter si agents se bloquent encore
```

**Thoughts de meilleure qualitÃ©:**

- Utiliser Claude API pour 10-20% des donnÃ©es
- Affiner les heuristiques selon failure modes observÃ©s

### Phase 3: ReAct-Full (Semaine 4+)

Ajouter:
- **Memory** entre subgoals
- **Multi-step replanning** (pas seulement un subgoal)
- **Observation sophistiquÃ©e** (objs visibles, distances, etc.)

---

## ğŸ› Troubleshooting

### Erreur: "Module seq2seq_react_light not found"

```bash
# VÃ©rifier que le fichier est bien lÃ 
ls seq2seq_react_light.py

# VÃ©rifier l'import
cd ~/Bureau/Alfred/alfred
python -c "import sys; sys.path.insert(0, '../alfred_experiments'); from seq2seq_react_light import ReActLightModule"
```

### Training trÃ¨s lent

```bash
# RÃ©duire batch size
# Dans configs/react_light_v1.yaml:
batch: 2  # au lieu de 4
```

### GPU Out of Memory

```bash
# RÃ©duire dhid
dhid: 64  # au lieu de 128
max_subgoals: 3  # au lieu de 5
```

### Success rate toujours 0%

VÃ©rifier:
1. âœ“ ModÃ¨le charge bien? â†’ `cat logs/train.log | grep "REACT"`
2. âœ“ Eval avec environnement? â†’ doit utiliser `eval_seq2seq.py`
3. âœ“ Replanning activÃ©? â†’ voir thoughts dans output

---

## ğŸ“ Support & Next Steps

### Obtenir de l'aide

1. **VÃ©rifier logs:** `experiments/*/logs/train.log`
2. **Comparer configs:** `diff configs/test_quick_gpu.yaml configs/react_light_v1.yaml`
3. **Tester import:** `python -c "from seq2seq_react_light import Module"`

### Prochaines questions Ã  se poser

- **Success rate obtenu?** â†’ Si <15%, tuner hyperparams
- **Failure modes?** â†’ Analyser oÃ¹ Ã§a Ã©choue (navigation? pickup?)
- **Replanning efficace?** â†’ Taux de rÃ©cupÃ©ration aprÃ¨s erreur?

### Ressources

- **Paper ReAct:** Yao et al. 2022 ([arXiv](https://arxiv.org/abs/2210.03629))
- **ALFRED:** [GitHub](https://github.com/askforalfred/alfred)
- **Votre baseline:** `seq2seq_cot.py` (rÃ©fÃ©rence)

---

## âœ… Checklist de DÃ©marrage

- [ ] Fichiers copiÃ©s (`seq2seq_react_light.py`, `react_light_v1.yaml`)
- [ ] Import fonctionne (`python -c "from seq2seq_react_light import Module"`)
- [ ] Training lancÃ© (`python scripts/run_experiment.py --config configs/react_light_v1.yaml`)
- [ ] Logs vÃ©rifiÃ©s (`cat experiments/*/logs/train.log`)
- [ ] Ã‰valuation faite (`eval_seq2seq.py`)
- [ ] Success rate mesurÃ© (objectif: >20%)

**Bon courage! ğŸš€**

---

## ğŸ“§ Questions FrÃ©quentes

**Q: Dois-je gÃ©nÃ©rer les thoughts avant le premier training?**  
R: Non, le modÃ¨le fonctionne sans (mode dÃ©gradÃ©). GÃ©nÃ©rez-les aprÃ¨s pour amÃ©liorer.

**Q: Combien de temps pour voir des rÃ©sultats?**  
R: 3 epochs (~2h) suffisent pour voir si Ã§a marche. Success rate visible aprÃ¨s eval.

**Q: Et si success rate reste 0%?**  
R: Normal en dÃ©but. VÃ©rifiez que:
1. Replanning est activÃ© (voir thoughts)
2. Ã‰valuation utilise l'environnement (pas juste inference)
3. ModÃ¨le a convergÃ© (CoT accuracy >90%)

**Q: DiffÃ©rence CoT vs ReAct?**  
R: CoT = plan fixe. ReAct = plan + observation + adaptation. CoT Ã©choue car pas de feedback, ReAct rÃ©cupÃ¨re des erreurs.